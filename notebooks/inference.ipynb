{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ebe898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af6d3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "asc_model_path = \"../models/asc_model/checkpoint-885\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "asc_model = AutoModelForSequenceClassification.from_pretrained(asc_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09fdb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect: card → neutral\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I charge it at night and skip taking the cord with me because of the good battery life.\"\n",
    "aspects = [\"card\"]\n",
    "\n",
    "for aspect in aspects:\n",
    "    inputs = tokenizer(\n",
    "        sentence,\n",
    "        aspect,   \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    outputs = asc_model(**inputs)\n",
    "    predicted_class = outputs.logits.argmax(dim=-1).item()\n",
    "    \n",
    "    label_map = {3 :\"positive\", 1: \"negative\", 2: \"neutral\"}  \n",
    "    print(f\"Aspect: {aspect} → {label_map[predicted_class]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bb4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hard disk': 'negative'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "asc_model_path = \"../models/asc_model/checkpoint-885\"\n",
    "ate_model_path = \"../models/ate_model\"\n",
    "\n",
    "asc_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "\n",
    "ate_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=ate_model_path,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "asc_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=asc_model_path,\n",
    "    tokenizer=asc_tokenizer\n",
    ")\n",
    "\n",
    "label_map = {\n",
    "    1: \"negative\",\n",
    "    2: \"neutral\",\n",
    "    3: \"positive\"\n",
    "}\n",
    "\n",
    "def analyze_sentence(sentence):\n",
    "    aspects = ate_pipeline(sentence)\n",
    "    results = {}\n",
    "\n",
    "    for asp in aspects:\n",
    "        aspect_term = asp['word']\n",
    "\n",
    "        input_text = f\"Aspect: {aspect_term}. Sentence: {sentence}\"\n",
    "        prediction = asc_pipeline(input_text)[0]\n",
    "\n",
    "        predicted_class = int(prediction['label'].replace(\"LABEL_\", \"\"))\n",
    "        polarity = label_map.get(predicted_class, \"unknown\")\n",
    "\n",
    "        results[aspect_term] = polarity\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "sentence = \"this shit hard disk is so slow.\"\n",
    "print(analyze_sentence(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cacc8c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: The design looks nice and the performance is fast.\n",
      "Extracted Aspects: ['performance']\n",
      "Aspect: performance → positive\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "\n",
    "# Load ATE model\n",
    "MODEL_ATE_PATH = \"../models/ate_model\"\n",
    "ate_tokenizer = AutoTokenizer.from_pretrained(MODEL_ATE_PATH)\n",
    "ate_model = AutoModelForTokenClassification.from_pretrained(MODEL_ATE_PATH)\n",
    "id2label = ate_model.config.id2label\n",
    "\n",
    "# -------------------------------\n",
    "# Load ASC model \n",
    "ASC_MODEL_PATH = \"../models/asc_model/checkpoint-885\"\n",
    "asc_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "asc_model = AutoModelForSequenceClassification.from_pretrained(ASC_MODEL_PATH)\n",
    "\n",
    "# Step 1: Extract Aspects (ATE)\n",
    "def extract_aspects(text):\n",
    "    tokens = ate_tokenizer(text.split(), is_split_into_words=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = ate_model(**tokens)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "    word_ids = tokens.word_ids()\n",
    "    aspects, current = [], []\n",
    "\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            continue\n",
    "        token = tokens.tokens()[idx]\n",
    "        label = id2label[predictions[0][idx].item()]\n",
    "\n",
    "        if label.startswith(\"B-Aspect\"):\n",
    "            if current:\n",
    "                aspects.append(\" \".join(current))\n",
    "                current = []\n",
    "            current.append(token)\n",
    "        elif label.startswith(\"I-Aspect\"):\n",
    "            current.append(token)\n",
    "        else:\n",
    "            if current:\n",
    "                aspects.append(\" \".join(current))\n",
    "                current = []\n",
    "\n",
    "    if current:\n",
    "        aspects.append(\" \".join(current))\n",
    "\n",
    "    aspects = [asp.replace(\"##\", \"\") for asp in aspects]\n",
    "    return aspects\n",
    "\n",
    "# Step 2: Classify Sentiment (ASC)\n",
    "def classify_sentiment(sentence, aspect):\n",
    "    inputs = asc_tokenizer(\n",
    "        sentence,\n",
    "        aspect,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = asc_model(**inputs)\n",
    "    predicted_class = outputs.logits.argmax(dim=-1).item()\n",
    "\n",
    "    \n",
    "    label_map = {\n",
    "        1: \"negative\",\n",
    "        2: \"neutral\",\n",
    "        3: \"positive\"\n",
    "    }\n",
    "    return label_map[predicted_class]\n",
    "\n",
    "\n",
    "# Demo\n",
    "if __name__ == \"__main__\":\n",
    "    sentence = \"The design looks nice and the performance is fast.\"\n",
    "\n",
    "    print(\"\\nSentence:\", sentence)\n",
    "\n",
    "    # Run ATE\n",
    "    aspects = extract_aspects(sentence)\n",
    "    print(\"Extracted Aspects:\", aspects)\n",
    "\n",
    "    # Run ASC\n",
    "    for aspect in aspects:\n",
    "        sentiment = classify_sentiment(sentence, aspect)\n",
    "        print(f\"Aspect: {aspect} → {sentiment}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
